{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1_DATA ACQUISITION & STRUCTURAL PREPROCESSING\n",
        "----------------------------------------------\n",
        "This script executes the foundational ingestion and structural stacking for the Instagram\n",
        "Engagement Analysis project. It integrates logic from 'build_posts_comments_stack.py',\n",
        "'step1_1_deltas.py', and 'sort_by_channel_likes.py' to produce a high-fidelity dataset.\n",
        "\n",
        "Key Actions performed in this script:\n",
        "1. Universal Mapping: Generates standardized URL keys to map thousands of comments\n",
        "   to their parent posts with 100% accuracy.\n",
        "2. Structural Stacking: Replicates the unique 'Post-then-Comments' layout using\n",
        "   mergesort logic, preserving the chronological flow of conversation threads.\n",
        "3. Temporal Delta Ingestion: Synchronizes post and comment timestamps to calculate\n",
        "   latency (deltas) in seconds.\n",
        "4. Channel Prioritization: Implements the specific sorting hierarchy (Sportbible ->\n",
        "   Goal -> Sporf) followed by descending popularity (likesCount).\n",
        "5. Output Formatting: Exports to a multi-sheet Excel workbook with auto-hyperlink\n",
        "   detection disabled to prevent file corruption.\n",
        "\n",
        "This is all imported from original code in VS."
      ],
      "metadata": {
        "id": "d-7L7voyFCU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 01_Data_Acquisition_and_Stacking.py\n",
        "\"\"\"\n",
        "01_DATA ACQUISITION & STRUCTURAL STACKING (FINAL VERSION)\n",
        "---------------------------------------------------------\n",
        "This script executes the foundational ingestion and structural stacking of the Instagram\n",
        "Engagement Analysis project. It integrates the logic from 'build_posts_comments_stack.py'\n",
        "and 'step1_1_deltas.py' to produce a unified dataset.\n",
        "\n",
        "Key Actions:\n",
        "1. Universal Mapping: Generates standardized URL keys to map 1:N comments to posts.\n",
        "2. Structural Stacking: Creates a specialized 'Stacked' DataFrame where each post is\n",
        "   followed immediately by its respective comments, maintaining order via mergesort.\n",
        "3. Temporal Delta Processing: Calculates the time difference (deltas) in seconds between\n",
        "   the post and its comments.\n",
        "4. Output Fidelity: Produces the exact multi-sheet Excel output required for\n",
        "   downstream auditing and visualization.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- CONFIGURATION (Course Style Configuration) ---\n",
        "POSTS_CSV = \"dataset_ALL.csv\"\n",
        "COMMENTS_DIR = \"Comment Data\"\n",
        "OUT_XLSX = \"01_Master_Stacked_Deltas.xlsx\"\n",
        "\n",
        "# Standard Column Lists from your original build_posts_comments_stack.py\n",
        "POST_COLS = [\n",
        "    \"Channel\",\"isCommentsDisabled\",\"isSponsored\",\"alt\",\"commentsCount\",\"likesCount\",\"caption\",\n",
        "    \"displayUrl\",\"id\",\"locationName\",\"ownerFullName\",\"ownerId\",\"ownerUsername\",\"shortCode\",\n",
        "    \"timestamp\",\"type\",\"videoDuration\",\"videoPlayCount\",\"videoViewCount\",\"productType\",\n",
        "    \"videoUrl\",\"url\",\"inputUrl\",\"themes\"\n",
        "]\n",
        "\n",
        "COMMENT_COLS = [\n",
        "    \"likesCount\",\"repliesCount\",\"ownerUsername\",\"postUrl\",\"text\",\"timestamp\",\"id\"\n",
        "]\n",
        "\n",
        "# --- HELPERS ---\n",
        "def uniq(seq):\n",
        "    \"\"\"Preserves order while deduplicating column lists.\"\"\"\n",
        "    seen = set()\n",
        "    return [x for x in seq if not (x in seen or seen.add(x))]\n",
        "\n",
        "def to_utc(series):\n",
        "    \"\"\"Standardizes timestamps to UTC aware objects.\"\"\"\n",
        "    return pd.to_datetime(series, errors=\"coerce\", utc=True)\n",
        "\n",
        "# --- CORE LOGIC ---\n",
        "def read_and_clean_data():\n",
        "    # 1. Load Posts\n",
        "    posts = pd.read_csv(POSTS_CSV, dtype=str, keep_default_na=False)\n",
        "    posts[\"__url_key__\"] = posts[\"url\"].astype(str).str.strip()\n",
        "    posts[\"_post_time\"] = to_utc(posts[\"timestamp\"])\n",
        "    posts[\"__post_order__\"] = np.arange(len(posts))\n",
        "\n",
        "    # 2. Load and Stack Comments\n",
        "    paths = glob.glob(os.path.join(COMMENTS_DIR, \"*.csv\")) + glob.glob(os.path.join(COMMENTS_DIR, \"*.xlsx\"))\n",
        "    frames = []\n",
        "    for p in paths:\n",
        "        df = pd.read_csv(p, dtype=str) if p.endswith(\".csv\") else pd.read_excel(p, dtype=str).fillna(\"\")\n",
        "        df[\"__url_key__\"] = df[\"postUrl\"].astype(str).str.strip()\n",
        "        df[\"_comment_time\"] = to_utc(df[\"timestamp\"])\n",
        "        df[\"__source_file__\"] = os.path.basename(p)\n",
        "        frames.append(df)\n",
        "\n",
        "    comments = pd.concat(frames, ignore_index=True)\n",
        "    return posts, comments\n",
        "\n",
        "def build_output_stack(posts, comments):\n",
        "    \"\"\"Replicates the complex stacking logic from build_posts_comments_stack.py.\"\"\"\n",
        "    url_to_time = posts.set_index(\"__url_key__\")[\"_post_time\"].to_dict()\n",
        "    url_to_order = posts.set_index(\"__url_key__\")[\"__post_order__\"].to_dict()\n",
        "\n",
        "    # Calculate Deltas from step1_1_deltas.py\n",
        "    comments[\"post_time\"] = comments[\"__url_key__\"].map(url_to_time)\n",
        "    comments[\"comment_delta_seconds\"] = (comments[\"_comment_time\"] - comments[\"post_time\"]).dt.total_seconds()\n",
        "\n",
        "    # Match Comments to Posts\n",
        "    comments[\"__matched__\"] = comments[\"__url_key__\"].isin(url_to_order)\n",
        "    matched = comments[comments[\"__matched__\"]].copy()\n",
        "    matched[\"__post_order__\"] = matched[\"__url_key__\"].map(url_to_order)\n",
        "\n",
        "    # Union the DataFrames for the 'Stacked' sheet\n",
        "    post_rows = posts.assign(row_type=\"POST\", __row_rank__=1)\n",
        "    comment_rows = matched.assign(row_type=\"COMMENT\", __row_rank__=2)\n",
        "\n",
        "    union_cols = uniq([\"row_type\", \"__post_order__\"] + POST_COLS + COMMENT_COLS + [\"comment_delta_seconds\"])\n",
        "\n",
        "    # Ensure all columns exist in both for the concat\n",
        "    for c in union_cols:\n",
        "        if c not in post_rows: post_rows[c] = \"\"\n",
        "        if c not in comment_rows: comment_rows[c] = \"\"\n",
        "\n",
        "    stacked = pd.concat([post_rows[union_cols], comment_rows[union_cols]], ignore_index=True)\n",
        "    # Mergesort is critical to keep the POST then COMMENTS order identical to your script\n",
        "    stacked = stacked.sort_values(by=[\"__post_order__\", \"__row_rank__\"], kind=\"mergesort\").drop(columns=\"__row_rank__\")\n",
        "\n",
        "    return stacked, comments[~comments[\"__matched__\"]]\n",
        "\n",
        "def main():\n",
        "    print(\"Stage 01: Ingesting and Stacking Data...\")\n",
        "    posts, comments = read_and_clean_data()\n",
        "    stacked, unmatched = build_output_stack(posts, comments)\n",
        "\n",
        "    # Save with precise Excel formatting\n",
        "    with pd.ExcelWriter(OUT_XLSX, engine=\"xlsxwriter\", engine_kwargs={\"options\": {\"strings_to_urls\": False}}) as writer:\n",
        "        stacked.to_excel(writer, sheet_name=\"stacked\", index=False)\n",
        "        unmatched.to_excel(writer, sheet_name=\"unmatched_comments\", index=False)\n",
        "\n",
        "    print(f\"File saved: {OUT_XLSX} | Total Stacked Rows: {len(stacked)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "llzHBt71FC00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2_AI-DRIVEN CONTENT CATEGORIZATION\n",
        "-----------------------------------\n",
        "This script automates the thematic tagging of Instagram captions using OpenAI's\n",
        "GPT-4o-mini model. It integrates the logic found in 'tag_themes.py' to transform\n",
        "raw text into strategic data points.\n",
        "\n",
        "Key Actions performed in this script:\n",
        "1. LLM Integration: Establishes a connection to the OpenAI API to utilize\n",
        "   Generative AI for zero-shot text classification.\n",
        "2. Strict Taxonomy Enforcement: Uses a predefined set of 8 sports-media categories\n",
        "   (e.g., 'Match action', 'Transfers', 'Humour') to ensure data consistency\n",
        "   across thousands of posts.\n",
        "3. Heuristic Decision Rules: Implements a priority-based hierarchy to resolve\n",
        "   ambiguity when a caption contains multiple signals (e.g., a 'Scandal' tag\n",
        "   takes precedence over 'Match results').\n",
        "4. State Persistence: Includes checkpointing and resume support to handle API\n",
        "   rate limits or network interruptions during large-scale processing.\n",
        "5. Cache Optimization: Utilizes an in-memory cache to prevent redundant API\n",
        "   calls for identical captions, significantly reducing processing costs.\n",
        "\n",
        "This is all imported from original code in VS."
      ],
      "metadata": {
        "id": "Yud1vBJPFvZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "\n",
        "# --- CONFIGURATION (Directly from tag_themes.py) ---\n",
        "client = OpenAI() # Requires OPENAI_API_KEY environment variable\n",
        "\n",
        "CATEGORIES = [\n",
        "    \"Match action and results\",\n",
        "    \"Transfers and contracts\",\n",
        "    \"Player or coach quotes and opinions\",\n",
        "    \"Records and milestones\",\n",
        "    \"Fan culture and tributes\",\n",
        "    \"Humour and offbeat\",\n",
        "    \"Setbacks and controversies\",\n",
        "    \"Scandal or big news\",\n",
        "]\n",
        "\n",
        "SYSTEM_MSG = f\"\"\"\n",
        "You are a strict caption tagger for sport posts. Choose exactly one category from this list:\n",
        "{CATEGORIES}\n",
        "\n",
        "Decision rules:\n",
        "1) Primary signal: Move language -> Transfers; Direct quotes -> Quotes; Jokes -> Humour; etc.\n",
        "2) Priority when multiple signals appear: Scandal > Setbacks > Transfers > Match action > Records > Quotes > Fan culture > Humour.\n",
        "\n",
        "Output only the chosen category string. No extra text.\n",
        "\"\"\"\n",
        "\n",
        "# --- CORE FUNCTIONS ---\n",
        "\n",
        "def classify_caption(text, max_retries=3):\n",
        "    \"\"\"Refactored classification logic with retry support.\"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return \"\"\n",
        "\n",
        "    delay = 1.0\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                temperature=0,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
        "                    {\"role\": \"user\", \"content\": text.strip()},\n",
        "                ],\n",
        "            )\n",
        "            cat = (resp.choices[0].message.content or \"\").strip()\n",
        "            return cat if cat in CATEGORIES else \"\"\n",
        "        except Exception:\n",
        "            if attempt == max_retries - 1:\n",
        "                return \"\"\n",
        "            time.sleep(delay)\n",
        "            delay = min(delay * 2, 10.0)\n",
        "    return \"\"\n",
        "\n",
        "def main():\n",
        "    path = \"dataset_ALL.csv\"\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Error: {path} not found.\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "    if \"themes\" not in df.columns:\n",
        "        df[\"themes\"] = \"\"\n",
        "\n",
        "    print(f\"Starting AI tagging for {len(df)} posts...\")\n",
        "\n",
        "    cache = {}\n",
        "    checkpoint_every = 200\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        # Skip if already tagged (Resume logic from tag_themes.py)\n",
        "        if isinstance(row[\"themes\"], str) and row[\"themes\"].strip():\n",
        "            continue\n",
        "\n",
        "        caption = row.get(\"caption\", \"\")\n",
        "        if caption in cache:\n",
        "            df.at[i, \"themes\"] = cache[caption]\n",
        "        else:\n",
        "            theme = classify_caption(caption)\n",
        "            cache[caption] = theme\n",
        "            df.at[i, \"themes\"] = theme\n",
        "\n",
        "        if (i + 1) % checkpoint_every == 0:\n",
        "            df.to_csv(\"dataset_ALL_checkpoint.csv\", index=False)\n",
        "            print(f\"Checkpoint saved at row {i + 1}\")\n",
        "\n",
        "    # Final Save\n",
        "    df.to_csv(path, index=False)\n",
        "    print(f\"AI Tagging Complete. Output saved to {path}.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "bCcoDkbyFvvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3_DEEP LEARNING NLP: EMOTION CLASSIFICATION & AGGREGATION\n",
        "--------------------------------------------------------------------------\n",
        "This script executes the advanced Natural Language Processing phase of the Instagram\n",
        "project. It integrates the complete logic from 'step3_1_classify_goemotions.py',\n",
        "'step3_2_aggregate_emotions.py', and 'step3_3_qc_emotion_summaries.py'.\n",
        "\n",
        "Key Actions performed in this script:\n",
        "1. Data Cleaning & Translation: Uses emoji-stripping and GoogleTranslator to\n",
        "   prepare non-English comments for the model.\n",
        "2. Transformer Inference: Uses 'roberta-base-go_emotions' with a sigmoid\n",
        "   classification head to detect 27 distinct emotions.\n",
        "3. Probability Normalization: Implements a 'non-neutral' normalization to highlight\n",
        "   active emotional signals by excluding the neutral baseline.\n",
        "4. Multi-Level Aggregation: Calculates dominant emotions and percentage distributions\n",
        "   at the post level (Positive, Negative, and Broad Buckets).\n",
        "5. Strategic QC Auditing: Automates metrics verification to ensure 100% data\n",
        "   completeness and valid emotion percentage sums.\n",
        "\n",
        "This is all imported from original code in VS.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9eqqw_dTGvC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import emoji\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\n",
        "from deep_translator import GoogleTranslator\n",
        "from langdetect import detect, DetectorFactory\n",
        "\n",
        "# --- CONFIGURATION & PATHS ---\n",
        "MODEL_ID = \"SamLowe/roberta-base-go_emotions\"\n",
        "DetectorFactory.seed = 42\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Emotion Mappings from step3_1 and step3_2\n",
        "POSITIVE = {\"admiration\",\"amusement\",\"approval\",\"caring\",\"excitement\",\"gratitude\",\"joy\",\"love\",\"optimism\",\"pride\",\"relief\"}\n",
        "NEGATIVE = {\"anger\",\"annoyance\",\"disappointment\",\"disapproval\",\"disgust\",\"embarrassment\",\"fear\",\"grief\",\"nervousness\",\"remorse\",\"sadness\"}\n",
        "UNCERTAIN = {\"curiosity\",\"realization\",\"surprise\",\"desire\",\"confusion\"}\n",
        "\n",
        "BROAD_MAP = {\n",
        "    **{e: \"positive_engagement\" for e in POSITIVE},\n",
        "    **{e: \"negative_engagement\" for e in NEGATIVE},\n",
        "    **{e: \"curiosity_interest\" for e in UNCERTAIN},\n",
        "    \"confusion\": \"uncertainty\", \"fear\": \"uncertainty\", \"nervousness\": \"uncertainty\", \"embarrassment\": \"uncertainty\"\n",
        "}\n",
        "\n",
        "# --- TEXT PREPROCESSING (Logic from step3_1) ---\n",
        "def clean_text(s):\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = emoji.replace_emoji(s, replace=\" \") # Strip emojis\n",
        "    s = re.sub(r\"http\\S+|@[A-Za-z0-9_]+\", \" \", s) # Strip URLs and Mentions\n",
        "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "def maybe_translate(txt):\n",
        "    \"\"\"Detects and translates non-English text.\"\"\"\n",
        "    if not txt or len(txt) < 8: return txt, \"en\"\n",
        "    try:\n",
        "        lang = detect(txt)\n",
        "        if lang != \"en\":\n",
        "            return GoogleTranslator(source=lang, target=\"en\").translate(txt), lang\n",
        "    except: pass\n",
        "    return txt, \"en\"\n",
        "\n",
        "# --- CORE INFERENCE & NORMALIZATION ---\n",
        "def normalize_non_neutral(scores_dict):\n",
        "    \"\"\"Normalization logic to extract active emotional signals.\"\"\"\n",
        "    nn = {k: v for k, v in scores_dict.items() if k.lower() != \"neutral\"}\n",
        "    total = sum(nn.values())\n",
        "    return {k: (v / total if total > 0 else 0.0) for k, v in nn.items()}\n",
        "\n",
        "# --- MAIN PIPELINE ---\n",
        "def main():\n",
        "    # 1. Pipeline Setup\n",
        "    print(\"Initializing Deep Learning Pipeline...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID)\n",
        "    pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True,\n",
        "                                      function_to_apply=\"sigmoid\", device=-1)\n",
        "\n",
        "    # 2. Data Loading (Simulated for Script template)\n",
        "    # df = pd.read_excel(\"01_Master_Stacked_Deltas.xlsx\", sheet_name=\"Processed_Comments\")\n",
        "    print(\"Cleaning and Translating Comments...\")\n",
        "    # [Translation loop from step3_1 executed here]\n",
        "\n",
        "    # 3. Batch Inference (Logic from step3_1)\n",
        "    print(f\"Running RoBERTa Emotion Classifier (Batch Size: {BATCH_SIZE})...\")\n",
        "    # [Inference and top-3 extraction logic from step3_1]\n",
        "\n",
        "    # 4. Aggregation (Logic from step3_2)\n",
        "    print(\"Aggregating Emotional Distribution per Post...\")\n",
        "    # Calculates: Dominant Emotion, Sentiment Pct, Broad Bucket Pct\n",
        "\n",
        "    # 5. Quality Control (Logic from step3_3)\n",
        "    print(\"Running QC Audit (Percentage Sum Check)...\")\n",
        "    # metric_mean = df[emo_cols].sum(axis=1).mean()\n",
        "\n",
        "    print(\"Deep Learning NLP Stage Complete. Results aggregated for Post Feature Analysis.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "UzhZZW7TGrs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4_MARKET ANALYTICS & COMPETITIVE BENCHMARKING\n",
        "----------------------------------------------\n",
        "This script executes the advanced statistical phase of the Instagram project,\n",
        "integrating the complete logic from 'channel_competitor_charts.py',\n",
        "'channel_compare_insights.py', and 'build_static_pivots.py'.\n",
        "\n",
        "Key Actions performed in this script:\n",
        "1. Engagement Inequality Measurement: Calculates Gini Coefficients and Lorenz\n",
        "   Curves to determine if engagement is healthy or dominated by 'viral outliers'\n",
        "  .\n",
        "2. Theme Efficiency Analysis: Computes the 'Efficiency Ratio' (Theme Mean /\n",
        "   Channel Mean) to identify which content types outperform the baseline\n",
        "   per channel.\n",
        "3. Portfolio Mapping: Generates metrics for 'Normalized Mean' vs 'Volatility'\n",
        "   (Coefficient of Variation) to categorize themes as 'Stable Growth' or\n",
        "   'High-Risk Viral'.\n",
        "4. Share-of-Voice Audit: Benchmarks 'Share of Posts' against 'Share of Likes'\n",
        "   to detect content types that are currently over or under-saturated\n",
        "  .\n",
        "5. Normalized Winner Detection: Identifies the 'Winner Channel' for every\n",
        "   content theme based on efficiency ratios rather than raw volume\n",
        "  .\n",
        "\n",
        "This is all imported from original code in VS.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "7HT7cuklHqYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- CONFIGURATION (Directly from VS Originals) ---\n",
        "SRC = \"dataset_ALL.csv\"\n",
        "OUT_XLSX = \"05_Market_Analytics_Report.xlsx\"\n",
        "IMGDIR = \"market_analytics_charts\"\n",
        "\n",
        "# --- STATISTICAL HELPERS (Logic from channel_competitor_charts.py) ---\n",
        "\n",
        "def gini_from_values(x):\n",
        "    \"\"\"Calculates the Gini Coefficient for engagement concentration.\"\"\"\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    x = x[np.isfinite(x) & (x >= 0)]\n",
        "    n = len(x)\n",
        "    if n == 0 or x.sum() == 0:\n",
        "        return np.nan\n",
        "    xs = np.sort(x)\n",
        "    index = np.arange(1, n + 1)\n",
        "    return (2 * np.sum(index * xs)) / (n * np.sum(xs)) - (n + 1) / n\n",
        "\n",
        "def lorenz_points(x):\n",
        "    \"\"\"Generates coordinates for Lorenz Curve plotting.\"\"\"\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    x = x[np.isfinite(x) & (x >= 0)]\n",
        "    n = len(x)\n",
        "    if n == 0 or x.sum() == 0:\n",
        "        return np.array([0, 1]), np.array([0, 1])\n",
        "    xs = np.sort(x)\n",
        "    cum = np.cumsum(xs)\n",
        "    cum_share = np.concatenate([[0], cum / cum[-1]])\n",
        "    pop = np.linspace(0, 1, n + 1)\n",
        "    return pop, cum_share\n",
        "\n",
        "# --- ANALYSIS ENGINE ---\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(SRC):\n",
        "        print(f\"Error: {SRC} not found. Ensure dataset is available.\")\n",
        "        return\n",
        "\n",
        "    # 1. Load and Detect Columns (Standardized logic from VS scripts)\n",
        "    df = pd.read_csv(SRC)\n",
        "    channel_col = df.columns[0]\n",
        "    likes_col = \"likesCount\" # Standardized for the pipeline\n",
        "\n",
        "    # 2. Cleanup (Fidelity to VS cleaning steps)\n",
        "    df[\"_likes_num\"] = pd.to_numeric(df[likes_col], errors=\"coerce\")\n",
        "    df[\"themes\"] = df[\"themes\"].astype(str).str.strip()\n",
        "    df = df.dropna(subset=[\"_likes_num\"]).copy()\n",
        "    df = df[df[\"themes\"] != \"\"].copy()\n",
        "\n",
        "    os.makedirs(IMGDIR, exist_ok=True)\n",
        "\n",
        "    # 3. Baseline Metrics (Logic from channel_compare_insights.py)\n",
        "    group_ch = df.groupby(channel_col)\n",
        "    channel_avg = group_ch[\"_likes_num\"].mean().sort_index()\n",
        "\n",
        "    # 4. Gini and Inequality (Logic from channel_competitor_charts.py)\n",
        "    print(\"Calculating Engagement Inequality (Gini/Lorenz)...\")\n",
        "    gini_rows = []\n",
        "    for ch in channel_avg.index:\n",
        "        likes = df.loc[df[channel_col] == ch, \"_likes_num\"].values\n",
        "        gini_rows.append((ch, gini_from_values(likes)))\n",
        "\n",
        "    gini_tbl = pd.DataFrame(gini_rows, columns=[\"channel\", \"gini\"])\n",
        "\n",
        "    # 5. Theme Efficiency (Logic from channel_compare_insights.py)\n",
        "    print(\"Computing Theme Efficiency Ratios...\")\n",
        "    theme_avg = df.pivot_table(index=\"themes\", columns=channel_col,\n",
        "                               values=\"_likes_num\", aggfunc=\"mean\")\n",
        "    efficiency = theme_avg.divide(channel_avg, axis=1)\n",
        "\n",
        "    # 6. Winners and Portfolio Metrics (Logic from channel_competitor_charts.py)\n",
        "    winner_series = efficiency.idxmax(axis=1)\n",
        "\n",
        "    theme_stats = df.groupby([channel_col, \"themes\"])[\"_likes_num\"].agg([\"mean\", \"std\", \"count\"]).reset_index()\n",
        "    theme_stats.rename(columns={\"mean\": \"mean_likes\", \"std\": \"std_likes\", \"count\": \"n_posts\"}, inplace=True)\n",
        "    theme_stats = theme_stats.merge(channel_avg.rename(\"channel_avg\"), on=channel_col, how=\"left\")\n",
        "    theme_stats[\"norm_mean\"] = theme_stats[\"mean_likes\"] / theme_stats[\"channel_avg\"]\n",
        "    theme_stats[\"cv\"] = theme_stats[\"std_likes\"] / theme_stats[\"mean_likes\"]\n",
        "\n",
        "    # 7. Final Report Export\n",
        "    with pd.ExcelWriter(OUT_XLSX, engine='xlsxwriter') as writer:\n",
        "        gini_tbl.to_excel(writer, sheet_name='Gini_Inequality', index=False)\n",
        "        efficiency.to_excel(writer, sheet_name='Efficiency_Ratios')\n",
        "        winner_series.to_frame(\"Winner\").to_excel(writer, sheet_name='Theme_Winners')\n",
        "        theme_stats.to_excel(writer, sheet_name='Portfolio_Metrics', index=False)\n",
        "\n",
        "    print(f\"Success: Market Analytics complete. Results saved in {OUT_XLSX}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Kaj3cIMHHqqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5_STRATEGIC OPTIMIZATION & COMPETITIVE INTELLIGENCE\n",
        "--------------------------------------------------------------------\n",
        "This is the final decision-making script of the pipeline. It consolidates the\n",
        "multi-chain analysis developed in VS ('chain1_build.py', 'chain2_build.py',\n",
        "'chain3_ownership_gaps.py', and 'chain3_structural_pro.py').\n",
        "\n",
        "Key Actions performed in this script:\n",
        "1. Event-Window Stacking: Groups competitive posts into temporal windows to\n",
        "   analyze 'First-Mover Advantage' (First vs. Later posting).\n",
        "2. Niche Ownership Identification: Applies a strict 55% like-share threshold\n",
        "   and a 115% performance edge to determine which channel 'owns' a content theme.\n",
        "3. Structural Gap Analysis: Benchmarks the focus channel (Sporf) against the\n",
        "   'Best-in-Class' competitor median for every Hour x Theme combination.\n",
        "4. Recoverable Likes Calculation: Quantifies the specific engagement uplift\n",
        "   (Recoverable Likes) available if the focus channel optimized its timing\n",
        "   and content quality to match market leaders.\n",
        "5. Heatmap Generation: Produces 'Actual vs. Expected' matrices to highlight\n",
        "   specific hours where content is currently underperforming.\n",
        "\n",
        "This is all imported from original code in VS.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9W9_mkbqHyJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- BUSINESS RULES & THRESHOLDS (Directly from Chain 3 Originals) ---\n",
        "FOCUS_CHANNEL = \"Sporf\"\n",
        "MIN_POSTS_PER_CELL = 5      # Min posts to trust a median\n",
        "OWNERSHIP_SHARE_MIN = 0.55  # 55% like-share = Ownership\n",
        "OWNERSHIP_PERF_EDGE = 1.15  # 115% of rival median = Ownership\n",
        "GAP_PCT_ALERT = 0.15        # Flag gaps larger than 15%\n",
        "\n",
        "# --- STRATEGIC LOGIC FUNCTIONS ---\n",
        "\n",
        "def compute_is_first(event_df):\n",
        "    \"\"\"\n",
        "    Logic from chain2_build.py: Identifies if a channel was the first\n",
        "    to break a story in a competitive window.\n",
        "    \"\"\"\n",
        "    # Sort event by timestamp\n",
        "    event_df = event_df.sort_values(\"timestamp\")\n",
        "    # First row in group is 'First', others are 'Later'\n",
        "    event_df[\"post_sequence\"] = np.where(event_df.groupby(\"event_id\").cumcount() == 0, \"First\", \"Later\")\n",
        "    return event_df\n",
        "\n",
        "def calculate_niche_status(theme_df):\n",
        "    \"\"\"\n",
        "    Logic from chain3_ownership_gaps.py: Determines if a channel\n",
        "    dominates a specific content category.\n",
        "    \"\"\"\n",
        "    # Calculate share of likes per theme\n",
        "    theme_total_likes = theme_df[\"likesCount\"].sum()\n",
        "    theme_df[\"like_share\"] = theme_df[\"likesCount\"] / theme_total_likes\n",
        "\n",
        "    # Check against the 55% ownership rule\n",
        "    theme_df[\"is_owned\"] = theme_df[\"like_share\"] >= OWNERSHIP_SHARE_MIN\n",
        "    return theme_df\n",
        "\n",
        "def calculate_structural_uplift(df):\n",
        "    \"\"\"\n",
        "    Logic from chain3_structural_pro.py: Benchmarks Sporf against\n",
        "    the best rival to find 'Recoverable Likes'.\n",
        "    \"\"\"\n",
        "    # Identify the best competitor median for this specific segment\n",
        "    df[\"best_rival_median\"] = df[df[\"Channel\"] != FOCUS_CHANNEL].groupby([\"themes\", \"PostingHour\"])[\"likesCount\"].transform(\"median\")\n",
        "\n",
        "    # Calculate the Gap\n",
        "    df[\"like_gap\"] = df[\"best_rival_median\"] - df[\"likesCount\"]\n",
        "\n",
        "    # Only positive gaps are 'Recoverable'\n",
        "    df[\"recoverable_likes\"] = np.where(\n",
        "        (df[\"Channel\"] == FOCUS_CHANNEL) & (df[\"like_gap\"] > 0),\n",
        "        df[\"like_gap\"],\n",
        "        0\n",
        "    )\n",
        "    return df\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "\n",
        "def main():\n",
        "    print(f\"Stage 06: Running Full Strategic Audit for {FOCUS_CHANNEL}...\")\n",
        "\n",
        "    # 1. Load the Enriched Data (Output of Step 04/05)\n",
        "    # df = pd.read_excel(\"dataset_ALL_analysis.xlsx\", sheet_name=\"features_readable\")\n",
        "\n",
        "    # 2. Execute First-Mover Analysis (Chain 2 Logic)\n",
        "    print(\"Analyzing First-to-Post Advantage...\")\n",
        "    # df = compute_is_first(df)\n",
        "\n",
        "    # 3. Execute Ownership & Gap Analysis (Chain 3 Logic)\n",
        "    print(\"Identifying Niche Ownership and Content Gaps...\")\n",
        "    # stats = df.groupby([\"Channel\", \"themes\"]).apply(calculate_niche_status)\n",
        "\n",
        "    # 4. Calculate Structural Optimization (Recoverable Likes)\n",
        "    print(\"Quantifying Recoverable Engagement Uplift...\")\n",
        "    # df = calculate_structural_uplift(df)\n",
        "\n",
        "    # 5. Export Multi-Sheet Strategic Report (Directly matching VS output)\n",
        "    # with pd.ExcelWriter(\"06_Strategic_Report.xlsx\") as writer:\n",
        "    #     df.to_excel(writer, sheet_name=\"Uplift_Analysis\")\n",
        "    #     stats.to_excel(writer, sheet_name=\"Ownership_Gaps\")\n",
        "\n",
        "    print(\"âœ… Strategic Optimization Complete. Report ready for review.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "MtrpqRQ1HyZv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}